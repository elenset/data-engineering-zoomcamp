{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4985cd9c-66b2-4ae7-af46-17367f2104fe",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a92e245-10a3-468a-999d-80e1609a914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c822be-76d9-46cf-bf4b-81bd9a45cc1f",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d50117e-430c-41d1-aafb-093b4ce5c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 83.65 seconds to send the entire dataset and flush\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from time import time\n",
    "\n",
    "def main():\n",
    "    # Create a Kafka producer\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    t0 = time()\n",
    "\t\n",
    "    csv_file = 'data/green_tripdata_2019-10.csv'  # change to your CSV file path if needed\n",
    "    \n",
    "    # We will only need these columns:\n",
    "    columns_to_read = ['lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount']\n",
    "    \n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        for row in reader:\n",
    "            selected_columns = {col: row[col] for col in columns_to_read}\n",
    "            # Send data to Kafka topic \"green-trips\"\n",
    "            producer.send('green-trips', value=selected_columns)\n",
    "\n",
    "    # Make sure any remaining messages are delivered\n",
    "    producer.flush()\n",
    "    \n",
    "    t1 = time()\n",
    "    print(f'It took {(t1 - t0):.2f} seconds to send the entire dataset and flush')\n",
    "\t\n",
    "    producer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168b07c-bfe9-452e-bc3a-62663503133d",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316d5b5-375d-44c7-889d-1b4e72188d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation_job.py\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.common.time import Duration\n",
    "\n",
    "def create_green_trips_source(t_env):\n",
    "    table_name = \"green_trips_source\"\n",
    "    pattern = \"yyyy-MM-dd HH:mm:ss\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            lpep_pickup_datetime VARCHAR,\n",
    "            lpep_dropoff_datetime VARCHAR,\n",
    "            PULocationID INTEGER,\n",
    "            DOLocationID INTEGER,\n",
    "            passenger_count INTEGER,\n",
    "            trip_distance DOUBLE,\n",
    "            tip_amount DOUBLE,\n",
    "\t\t\tevent_watermark AS TO_TIMESTAMP(lpep_pickup_datetime, '{pattern}'),\n",
    "            WATERMARK FOR event_watermark AS event_watermark - INTERVAL '5' SECOND\n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'green-trips',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json',\n",
    "\t        'json.fail-on-missing-field' = 'false', \n",
    "            'json.ignore-parse-errors' = 'true'     \n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name\n",
    "\n",
    "def create_green_trips_sink(t_env):\n",
    "    table_name = 'aggregated_trips'\n",
    "    sink_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            PULocationID INTEGER,\n",
    "            DOLocationID INTEGER,\n",
    "            session_start TIMESTAMP(3),\n",
    "            session_end TIMESTAMP(3),\n",
    "            num_trips BIGINT,\n",
    "\t\t\tsession_duration BIGINT,\n",
    "            PRIMARY KEY (PULocationID, DOLocationID, session_start) NOT ENFORCED\n",
    "        ) WITH (\n",
    "            'connector' = 'jdbc',\n",
    "            'url' = 'jdbc:postgresql://postgres:5432/postgres',\n",
    "            'table-name' = '{table_name}',\n",
    "            'username' = 'postgres',\n",
    "            'password' = 'postgres',\n",
    "            'driver' = 'org.postgresql.Driver'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(sink_ddl)\n",
    "    return table_name\n",
    "\n",
    "def log_aggregation():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.enable_checkpointing(60 * 1000)\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "\n",
    "    # Set up the table environment\n",
    "    settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "    t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "\t\n",
    "\t\n",
    "\n",
    "    try:\n",
    "        # Create Kafka table for green trips\n",
    "        source_table = create_green_trips_source(t_env)\n",
    "        aggregated_table = create_green_trips_sink(t_env)\n",
    "\n",
    "        t_env.execute_sql(f\"\"\"\n",
    "        INSERT INTO {aggregated_table}\n",
    "        SELECT\n",
    "            PULocationID,\n",
    "            DOLocationID,\n",
    "            SESSION_START(event_watermark, INTERVAL '5' MINUTE) AS session_start,\n",
    "            SESSION_END(event_watermark, INTERVAL '5' MINUTE) AS session_end,\n",
    "            COUNT(*) AS num_trips,\n",
    "\t\t\tTIMESTAMPDIFF(SECOND, SESSION_START(event_watermark, INTERVAL '5' MINUTE), SESSION_END(event_watermark, INTERVAL '5' MINUTE)) as session_duration\n",
    "        FROM {source_table} \n",
    "        GROUP BY PULocationID, DOLocationID, SESSION(event_watermark, INTERVAL '5' MINUTE)\n",
    "\t\tORDER BY session_duration DESC LIMIT 1;\n",
    "        \"\"\").wait()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Writing records from Kafka to JDBC failed:\", str(e))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_aggregation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a83278-86b6-48ac-99e3-72024dfb400c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
