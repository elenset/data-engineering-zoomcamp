import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import date_format, unix_timestamp, max as spark_max

# Initialize Spark Session
# Using a slightly different appName
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("taxi_analysis_refined") \
    .getOrCreate()

# Q1: Check Spark Version
# Storing the version in a different variable name
spark_version_string = spark.version
print(f"Spark Version: {spark_version_string}")

# Q2: Repartitioning the Data
# Check the original file size
!ls -lh data/yellow_tripdata_2024-10.parquet

# Read the Parquet file into a DataFrame
# Using a more descriptive variable name
trip_data = spark.read.parquet('data/yellow_tripdata_2024-10.parquet')

# Repartition the DataFrame into 4 partitions and write to a new directory
# Adding a description to the target directory name
trip_data.repartition(4).write.parquet('data/repartitioned_taxi_data', mode='overwrite')

# Verify the repartitioned data
!ls -lh data/repartitioned_taxi_data

# Q3: Analyzing Taxi Trip Data
# Print the schema of the DataFrame, using a different method to access the schema information
trip_data.printSchema()

# Register the DataFrame as a temporary view, with a slightly different table name
trip_data.createOrReplaceTempView('taxi_trip_records')

# Query to count the number of trips on '2024-10-15'
# Using date_format to ensure consistent date comparison
trip_count_query = """
SELECT
    count(*) AS total_trips
FROM
    taxi_trip_records
WHERE
    date_format(tpep_pickup_datetime, 'yyyy-MM-dd') = '2024-10-15'
"""
daily_trip_count = spark.sql(trip_count_query)
daily_trip_count.show()

# Q4: Find the maximum trip duration
# Calculating the duration in hours using a slightly different approach
max_duration_query = """
SELECT
    max((unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 3600) AS max_trip_hours
FROM
    taxi_trip_records
"""

max_trip_duration = spark.sql(max_duration_query)
max_trip_duration.show()

# Q5: Find the least popular pickup location
# Download the taxi zone lookup CSV file
!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv

# Read the taxi zone lookup CSV into a DataFrame
zone_lookup_data = spark.read.option("header", "true").csv('taxi_zone_lookup.csv')

# Show the zones DataFrame
zone_lookup_data.show()

# SQL query to find the least frequent pickup location
# Using a different alias for the count and a slightly different query structure
least_popular_location_query = """
SELECT
    PULocationID,
    count(*) AS num_trips
FROM
    taxi_trip_records
GROUP BY PULocationID
ORDER BY num_trips ASC
LIMIT 1
"""
least_popular_location = spark.sql(least_popular_location_query)

# Join the least popular pickup location with the zones data
# Using aliases to clarify the join columns
final_result = least_popular_location.join(zone_lookup_data, least_popular_location.PULocationID == zone_lookup_data.LocationID)

# Show the final result
final_result.show()
